{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wHEWgOeyowh",
        "outputId": "62779a15-44a1-461b-f30c-f5259278d1ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=64e88a33eaebab1786cae311d3f36603830af5b0aeee00a904b4b9b3626a4c2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "TQ1AXfu8xY_S",
        "outputId": "f2c55714-fae4-40c1-d7ef-6a499e9d6378"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-da14a01459da>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Open the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Extract all the contents into the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_to_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.zip'"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, min, avg, count, countDistinct, lit, sum, when, year, month, dayofmonth\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import DateType\n",
        "import pyspark.sql.functions as F\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Weather\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path to the zip file assuming the data.zip is in the same directory as this ipynb file\n",
        "zip_file_path = 'data.zip'\n",
        "\n",
        "# Directory where the zip file will be extracted\n",
        "extract_to_dir = 'data'\n",
        "\n",
        "# Check if the extraction directory exists, create if it doesn't\n",
        "if not os.path.exists(extract_to_dir):\n",
        "    os.makedirs(extract_to_dir)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents into the directory\n",
        "    zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "print(f\"Extracted all files in {zip_file_path} to {extract_to_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the data.zip has been extracted into a folder named 'data'\n",
        "df = spark.read.csv(\"data/data/*/*.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame schema to understand your data\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "pxzy2rdv3e9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max as pyspark_max, when, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Convert DATE column to date type\n",
        "df = df.withColumn('DATE', col('DATE').cast('date'))\n",
        "\n",
        "# Extract year from DATE column\n",
        "df = df.withColumn('YEAR', year('DATE'))\n",
        "\n",
        "# Filter out missing MAX values\n",
        "df_filtered = df.filter(df['MAX'] != 9999.9)\n",
        "\n",
        "# Find the hottest day for each year\n",
        "window_spec = Window.partitionBy('YEAR')\n",
        "hottest_df = hottest_days = df_filtered.withColumn('max_temp_rank', row_number().over(window_spec.orderBy(col('MAX').desc()))) \\\n",
        "                          .filter(col('max_temp_rank') == 1) \\\n",
        "                          .select('YEAR', 'STATION', 'NAME', 'DATE', 'MAX')\n",
        "\n",
        "# Show the hottest day for each year\n",
        "hottest_days.show(truncate=False)\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "result_pd_df = hottest_df.toPandas()\n",
        "\n"
      ],
      "metadata": {
        "id": "5TnB5Aql3nNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, min as pyspark_min, month\n",
        "\n",
        "# Convert DATE column to date type\n",
        "df = df.withColumn('DATE', col('DATE').cast('date'))\n",
        "\n",
        "# Extract month from DATE column\n",
        "df = df.withColumn('MONTH', month('DATE'))\n",
        "\n",
        "# Filter for January (month = 1)\n",
        "df_january = df.filter(df['MONTH'] == 1)\n",
        "\n",
        "# Filter out missing MIN values\n",
        "df_filtered = df_january.filter(df_january['MIN'] != 9999.9)\n",
        "\n",
        "# Find the coldest day for the month of January across all years\n",
        "window_spec = Window.partitionBy('MONTH')\n",
        "coldest_days = df_filtered.withColumn('min_temp_rank', row_number().over(window_spec.orderBy(col('MIN')))) \\\n",
        "                          .filter(col('min_temp_rank') == 1) \\\n",
        "                          .select('STATION', 'NAME', 'DATE', 'MIN')\n",
        "\n",
        "# Show the coldest day for the month of January across all years\n",
        "coldest_days.show(truncate=False)\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "result_pd_df = coldest_days.toPandas()\n",
        "\n"
      ],
      "metadata": {
        "id": "Fmbb8vqv5pB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Convert DATE column to date type\n",
        "df = df.withColumn('DATE', col('DATE').cast('date'))\n",
        "\n",
        "# Extract year from DATE column\n",
        "df = df.withColumn('YEAR', year('DATE'))\n",
        "\n",
        "# Filter for the year 2015\n",
        "df_2015 = df.filter(df['YEAR'] == 2015)\n",
        "\n",
        "# Filter out invalid precipitation values (99.99)\n",
        "df_filtered = df_2015.filter((df_2015['PRCP'] != 99.99))\n",
        "\n",
        "# Find any maximum and minimum precipitation for the year 2015\n",
        "max_precipitation_data = df_filtered.orderBy(col('PRCP').desc()).select('STATION', 'NAME', 'DATE', 'PRCP').limit(1)\n",
        "min_precipitation_data = df_filtered.orderBy(col('PRCP')).select('STATION', 'NAME', 'DATE', 'PRCP').limit(1)\n",
        "\n",
        "# Show the results\n",
        "print(\"Maximum precipitation for the year 2015 (excluding invalid values):\")\n",
        "max_precipitation_data.show(truncate=False)\n",
        "\n",
        "print(\"Minimum precipitation for the year 2015 (excluding invalid values):\")\n",
        "min_precipitation_data.show(truncate=False)"
      ],
      "metadata": {
        "id": "ZWTiC9Xj5-b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for the year 2019\n",
        "df_2019 = df.filter(year('DATE') == 2019)\n",
        "# Calculate the percentage of missing gust data for the year 2019\n",
        "percentage_missing = df_2019.filter(year(\"DATE\") == 2019) \\\n",
        "    .select((count(when(col(\"GUST\") == 999.9, True)) / count(\"*\") * 100).alias(\"MISSING GUST PERCENTAGE\"))\n",
        "\n",
        "percentage_missing.show()"
      ],
      "metadata": {
        "id": "o6klOaSS8LZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, year, month, mean, stddev, expr, desc\n",
        "\n",
        "# Filter for the year 2020\n",
        "df_2020 = df.filter(year('DATE') == 2020)\n",
        "\n",
        "# Filter out missing values (9999.9) for temperature (TEMP) column\n",
        "df_2020 = df_2020.filter(df_2020['TEMP'] != 9999.9)\n",
        "\n",
        "# Extract the month from the date column\n",
        "df_2020 = df_2020.withColumn('MONTH', month('DATE'))\n",
        "\n",
        "# Group by month and calculate mean, median, mode, and standard deviation for temperature\n",
        "result_df = df_2020.groupBy('MONTH') \\\n",
        "    .agg(mean('TEMP').alias('MEAN_TEMP'),\n",
        "         expr('percentile_approx(TEMP, 0.5)').alias('MEDIAN_TEMP'),\n",
        "         expr('max(TEMP) as MODE_TEMP'),  # Using max as an approximation for mode\n",
        "         stddev('TEMP').alias('STDDEV_TEMP')) \\\n",
        "    .orderBy('MONTH')\n",
        "\n",
        "# Display the results\n",
        "result_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "Il5nLKbe8Y2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "hottest_df_pd = hottest_df.toPandas()\n",
        "coldest_days_pd = coldest_days.toPandas()\n",
        "max_precipitation_data_pd = max_precipitation_data.toPandas()\n",
        "min_precipitation_data_pd = min_precipitation_data.toPandas()\n",
        "percentage_missing_pd = percentage_missing.toPandas()\n",
        "result_df_pd = result_df.toPandas()\n",
        "\n",
        "# Define the file path within Google Colab\n",
        "file_path = \"/result.txt\"\n",
        "\n",
        "# Formatting the output file\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(\"TASK 1:\\n\")\n",
        "    f.write(\"The hottest day for each year.\\n\")\n",
        "    f.write(tabulate(hottest_df_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "\n",
        "    f.write(\"\\n\\nTASK 2:\\n\")\n",
        "    f.write(\"The coldest day for the month of January across all years.\\n\")\n",
        "    f.write(tabulate(coldest_days_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "\n",
        "    f.write(\"\\n\\nTASK 3:\\n\")\n",
        "    f.write(\"Maximum Precipitation for 2015.\\n\")\n",
        "    f.write(tabulate(max_precipitation_data_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\nMinimum Precipitation for 2015.\\n\")\n",
        "    f.write(tabulate(min_precipitation_data_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "\n",
        "    f.write(\"\\n\\nTASK 4:\\n\")\n",
        "    f.write(\"Percentage of missing values for wind gust for the year 2019.\\n\")\n",
        "    f.write(tabulate(percentage_missing_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "\n",
        "    f.write(\"\\n\\nTASK 5:\\n\")\n",
        "    f.write(\"The mean, median, mode and standard deviation of the temperature for each month for the year 2020.\\n\")\n",
        "    f.write(tabulate(result_df_pd, headers='keys', tablefmt='psql', showindex=False))\n"
      ],
      "metadata": {
        "id": "R75-Te3Z-yVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}